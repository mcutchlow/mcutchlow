{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcutchlow/mcutchlow/blob/main/data-ethics-pa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z93YyOVXKYKz"
      },
      "source": [
        "###### University of Chicago\n",
        "###### DATA 25900: Ethics, Fairness, Responsibility, and Privacy in Data Science, Spring 2023\n",
        "###### Course Staff: Raul Castro Fernandez, Zhiru Zhu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHaswTTXKYK2"
      },
      "source": [
        "# Part 1: Inferential Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inZ7LiEFKYK2"
      },
      "source": [
        "## Problem 1.1\n",
        "\n",
        "A financial auditor is assessing the risk that different individual institutions across the country have taken while\n",
        "giving grants to people. The auditor uses an opaque, inaccessible machine learning model. They have applied this model\n",
        "to 10,000 records of loan data that represent loans made to adults in the US. It is unclear to us how the ML model\n",
        "works. All we know is that the model takes as input a row of the table and produces a label (`ml_risky_pred`) whose\n",
        "possible values are 1, 0, or -1. A value of 1 indicates that a loan was given to a person classified as 'risky'.\n",
        "A person classified as 'risky' is one who is not likely going to repay the loan according to the ML algorithm.\n",
        "A value of 0 indicates that the loan is not risky and that the person who received it is likely to repay the loan.\n",
        "In addition, in some cases the model produces a -1, which is to be interpreted as 'prediction unavailable'.\n",
        "From the perspective of a financial institution being assessed by this auditor, it is in their interest to have few\n",
        "loans that were granted to 'risky' people. That indicates that the companies' own risk models are working well.\n",
        "From the perspective of a state, it is in their interest that few financial institutions in their territory have\n",
        "granted few loans to 'risky' people. Doing so promotes economic stability.\n",
        "\n",
        "In `loans_data.csv`, you have all loan data available, along with the label provided by the black-box ML model. There is an accompanied documentation (`loans_data.md`) explaining what each attribute means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "d4cJClu8KYK2",
        "outputId": "da9e2368-2891-49aa-93c2-a0c280d6f24f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    emp_title  emp_length state homeownership  annual_income  \\\n",
              "0     global config engineer          3.0    NJ      MORTGAGE        90000.0   \n",
              "1      warehouse office clerk        10.0    HI          RENT        40000.0   \n",
              "2                    assembly         3.0    WI          RENT        40000.0   \n",
              "3            customer service         1.0    PA          RENT        30000.0   \n",
              "4        security supervisor         10.0    CA          RENT        35000.0   \n",
              "...                       ...         ...   ...           ...            ...   \n",
              "9995                   owner         10.0    TX          RENT       108000.0   \n",
              "9996                 director         8.0    PA      MORTGAGE       121000.0   \n",
              "9997                toolmaker        10.0    CT      MORTGAGE        67000.0   \n",
              "9998                  manager         1.0    WI      MORTGAGE        80000.0   \n",
              "9999       operations analyst         3.0    CT          RENT        66000.0   \n",
              "\n",
              "      verified_income  debt_to_income  annual_income_joint  \\\n",
              "0            Verified           18.01                  NaN   \n",
              "1        Not Verified            5.04                  NaN   \n",
              "2     Source Verified           21.15                  NaN   \n",
              "3        Not Verified           10.16                  NaN   \n",
              "4            Verified           57.96              57000.0   \n",
              "...               ...             ...                  ...   \n",
              "9995  Source Verified           22.28                  NaN   \n",
              "9996         Verified           32.38                  NaN   \n",
              "9997         Verified           45.26             107000.0   \n",
              "9998  Source Verified           11.99                  NaN   \n",
              "9999     Not Verified           20.82                  NaN   \n",
              "\n",
              "     verification_income_joint  debt_to_income_joint  ...  issue_month  \\\n",
              "0                          NaN                   NaN  ...     Mar-2018   \n",
              "1                          NaN                   NaN  ...     Feb-2018   \n",
              "2                          NaN                   NaN  ...     Feb-2018   \n",
              "3                          NaN                   NaN  ...     Jan-2018   \n",
              "4                     Verified                 37.66  ...     Mar-2018   \n",
              "...                        ...                   ...  ...          ...   \n",
              "9995                       NaN                   NaN  ...     Jan-2018   \n",
              "9996                       NaN                   NaN  ...     Feb-2018   \n",
              "9997           Source Verified                 29.57  ...     Feb-2018   \n",
              "9998                       NaN                   NaN  ...     Feb-2018   \n",
              "9999                       NaN                   NaN  ...     Feb-2018   \n",
              "\n",
              "      loan_status  initial_listing_status  disbursement_method   balance  \\\n",
              "0         Current                   whole                 Cash  27015.86   \n",
              "1         Current                   whole                 Cash   4651.37   \n",
              "2         Current              fractional                 Cash   1824.63   \n",
              "3         Current                   whole                 Cash  18853.26   \n",
              "4         Current                   whole                 Cash  21430.15   \n",
              "...           ...                     ...                  ...       ...   \n",
              "9995      Current                   whole                 Cash  21586.34   \n",
              "9996      Current                   whole                 Cash   9147.44   \n",
              "9997      Current              fractional                 Cash  27617.65   \n",
              "9998      Current                   whole                 Cash  21518.12   \n",
              "9999      Current                   whole                 Cash  11574.83   \n",
              "\n",
              "      paid_total  paid_principal  paid_interest  paid_late_fees  ml_risky_pred  \n",
              "0        1999.33          984.14        1015.19             0.0              0  \n",
              "1         499.12          348.63         150.49             0.0              1  \n",
              "2         281.80          175.37         106.43             0.0              0  \n",
              "3        3312.89         2746.74         566.15             0.0              0  \n",
              "4        2324.65         1569.85         754.80             0.0              0  \n",
              "...          ...             ...            ...             ...            ...  \n",
              "9995     2969.80         2413.66         556.14             0.0              0  \n",
              "9996     1456.31          852.56         603.75             0.0              1  \n",
              "9997     4620.80         2382.35        2238.45             0.0              0  \n",
              "9998     2873.31         2481.88         391.43             0.0              0  \n",
              "9999     1658.56         1225.17         433.39             0.0              0  \n",
              "\n",
              "[10000 rows x 56 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7e9b79e-5fc1-40da-8453-380371608337\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emp_title</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>state</th>\n",
              "      <th>homeownership</th>\n",
              "      <th>annual_income</th>\n",
              "      <th>verified_income</th>\n",
              "      <th>debt_to_income</th>\n",
              "      <th>annual_income_joint</th>\n",
              "      <th>verification_income_joint</th>\n",
              "      <th>debt_to_income_joint</th>\n",
              "      <th>...</th>\n",
              "      <th>issue_month</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>initial_listing_status</th>\n",
              "      <th>disbursement_method</th>\n",
              "      <th>balance</th>\n",
              "      <th>paid_total</th>\n",
              "      <th>paid_principal</th>\n",
              "      <th>paid_interest</th>\n",
              "      <th>paid_late_fees</th>\n",
              "      <th>ml_risky_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>global config engineer</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NJ</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>90000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>18.01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Mar-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>27015.86</td>\n",
              "      <td>1999.33</td>\n",
              "      <td>984.14</td>\n",
              "      <td>1015.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>warehouse office clerk</td>\n",
              "      <td>10.0</td>\n",
              "      <td>HI</td>\n",
              "      <td>RENT</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>5.04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>4651.37</td>\n",
              "      <td>499.12</td>\n",
              "      <td>348.63</td>\n",
              "      <td>150.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>assembly</td>\n",
              "      <td>3.0</td>\n",
              "      <td>WI</td>\n",
              "      <td>RENT</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>21.15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>fractional</td>\n",
              "      <td>Cash</td>\n",
              "      <td>1824.63</td>\n",
              "      <td>281.80</td>\n",
              "      <td>175.37</td>\n",
              "      <td>106.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>customer service</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PA</td>\n",
              "      <td>RENT</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>10.16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Jan-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>18853.26</td>\n",
              "      <td>3312.89</td>\n",
              "      <td>2746.74</td>\n",
              "      <td>566.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>security supervisor</td>\n",
              "      <td>10.0</td>\n",
              "      <td>CA</td>\n",
              "      <td>RENT</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>57.96</td>\n",
              "      <td>57000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>37.66</td>\n",
              "      <td>...</td>\n",
              "      <td>Mar-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>21430.15</td>\n",
              "      <td>2324.65</td>\n",
              "      <td>1569.85</td>\n",
              "      <td>754.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>owner</td>\n",
              "      <td>10.0</td>\n",
              "      <td>TX</td>\n",
              "      <td>RENT</td>\n",
              "      <td>108000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>22.28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Jan-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>21586.34</td>\n",
              "      <td>2969.80</td>\n",
              "      <td>2413.66</td>\n",
              "      <td>556.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>director</td>\n",
              "      <td>8.0</td>\n",
              "      <td>PA</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>121000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>32.38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>9147.44</td>\n",
              "      <td>1456.31</td>\n",
              "      <td>852.56</td>\n",
              "      <td>603.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>toolmaker</td>\n",
              "      <td>10.0</td>\n",
              "      <td>CT</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>67000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>45.26</td>\n",
              "      <td>107000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>29.57</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>fractional</td>\n",
              "      <td>Cash</td>\n",
              "      <td>27617.65</td>\n",
              "      <td>4620.80</td>\n",
              "      <td>2382.35</td>\n",
              "      <td>2238.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>manager</td>\n",
              "      <td>1.0</td>\n",
              "      <td>WI</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>11.99</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>21518.12</td>\n",
              "      <td>2873.31</td>\n",
              "      <td>2481.88</td>\n",
              "      <td>391.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>operations analyst</td>\n",
              "      <td>3.0</td>\n",
              "      <td>CT</td>\n",
              "      <td>RENT</td>\n",
              "      <td>66000.0</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>20.82</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>Feb-2018</td>\n",
              "      <td>Current</td>\n",
              "      <td>whole</td>\n",
              "      <td>Cash</td>\n",
              "      <td>11574.83</td>\n",
              "      <td>1658.56</td>\n",
              "      <td>1225.17</td>\n",
              "      <td>433.39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 56 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7e9b79e-5fc1-40da-8453-380371608337')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7e9b79e-5fc1-40da-8453-380371608337 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7e9b79e-5fc1-40da-8453-380371608337');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "loan_data = \"data/Part1/loans_data.csv\"\n",
        "df = pd.read_csv(loan_data)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyvNSZu_KYK3"
      },
      "source": [
        "#### 1.1.1: Suppose the dataset contains all relevant loans, i.e., the entire population of loans in the US. What's the percentage of current loans granted in Illinois that are risky? How does that compare with the whole US? If you find missing values, you can report them separately and you do not need to deal with them in this case.\n",
        "(*Note:* `Current` is a one of the categories of attribute `loan_status`; check the documentation for more info)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRwi_dAVKYK4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cytVjLhMKYK4"
      },
      "source": [
        "#### 1.1.2 Suppose the dataset does not represent all relevant loans, but instead only a sample of them. Repeat 1.1.1, but now knowing that the loans are a sample of the total population of loans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRkiSjoZKYK4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZGaP4qjKYK5"
      },
      "source": [
        "#### Regardless of reality, your boss wants to make the claim that the percentage of current risky loans in the state of Illinois is much lower than those deemed risky in the entire US. They want to take advantage of the situation to claim that financial institutions in the state of Illinois have better risk management than other states' institutions. Regardless of whether this claim is accurate and justifiable, the analyst produced the code you see below to make the point. Take a look at the code the analyst wrote:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V2VvAH3TKYK5"
      },
      "outputs": [],
      "source": [
        "population = df[(df['state'] == 'IL') & (df['loan_status'] == 'Current')]['ml_risky_pred']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k4qkcxJyKYK6",
        "outputId": "9a032e5e-b95c-449e-d4a2-c17b556e27b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found it:  10\n"
          ]
        }
      ],
      "source": [
        "sample_size = 50\n",
        "for i in range(500000):\n",
        "    sample = population.sample(sample_size, replace=False)\n",
        "    num_risky_loans = sum(sample)\n",
        "    if num_risky_loans <= 10:\n",
        "        print(\"Found it: \", str(num_risky_loans))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "e63I3MV8KYK6",
        "outputId": "96050304-7cc4-42d4-d3ec-9214055b112f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_proportion = num_risky_loans / len(sample)\n",
        "sample_proportion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BaEZTeKcKYK7",
        "outputId": "9cf569d9-836f-4aa3-9e90-23304490b313"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# random sample, so independence condition holds\n",
        "# testing success-failure condition (using the null value)\n",
        "mean = 0.5\n",
        "condition_one = sample_size * mean\n",
        "condition_two = sample_size * (1 - mean)\n",
        "condition_one > 10 and condition_two > 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NtWK7RJkKYK7",
        "outputId": "553372c5-5605-4197-d1a5-9bb843a91412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07071067811865475"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "# standard error of normal distribution for the p-value\n",
        "se = math.sqrt((mean*(1 - mean)) / sample_size)\n",
        "se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iyGVjADAKYK7"
      },
      "outputs": [],
      "source": [
        "# If the null hypothesis is true, then the null distribution follows a normal with:\n",
        "# mean = 0.5\n",
        "# se = 0.07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hZqeuHAkKYK7"
      },
      "outputs": [],
      "source": [
        "# Our point estimate is 0.2. We now find the tail area for that one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KF0K4A7bKYK7",
        "outputId": "14457094-4f51-4893-dabc-0a4aeb34665c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-4.242640687119285"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = (sample_proportion - mean) / se\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_uC-tQA2KYK8",
        "outputId": "d859d35f-4a9f-4986-9ffe-4385f85dd4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.2090496998585445e-05"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import scipy.stats\n",
        "\n",
        "# find p-value for two-tailed z-test\n",
        "p_value = scipy.stats.norm.sf(abs(z))*2\n",
        "p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xT6_yWs4KYK8",
        "outputId": "0adb68d0-ccc5-4ca4-a265-55e8b9678046"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_value < 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IriCVa3FKYK8"
      },
      "source": [
        "#### 1.1.3 The analyst claims they can reject the null hypothesis because p < 0.05. Furthermore, they defend they used a random sample so the independence condition holds. They use this evidence to claim that the percentage of risky loands in IL is lower than elsewhere. Having studied the code, provide a list of bullet points with the problems you see and that you would communicate to a decision maker to persuade them *not* to use this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6pS_cXkKYK8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTz4Wd7PKYK9"
      },
      "source": [
        "#### 1.1.4 What if you did not have access to the code or the data? Instead the analyst tells you that they found significant evidence that the number of risky loan in Illinois is below the national average. What would you have to do to test that claim?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDb6GnkKYK9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HczkFFx8KYK9"
      },
      "source": [
        "## Problem 1.2\n",
        "\n",
        "An analyst has run a number of experiments and obtained the following p-values for each. Correct for multiple comparisons using Bonferroni methods and False Discovery Rate. Assume alpha = 10%. *In this case, we ask you to write the code yourself, as opposed to using an existing library.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxHvNP6PKYK9"
      },
      "outputs": [],
      "source": [
        "p_values = [0.004, 0.44501577, 0.74140679, 0.0003, 0.0040743296,\n",
        "       0.40743933, 0.94285637, 0.00158846, 0.31936529, 0.70628362,\n",
        "       0.3215325 , 0.32070448, 0.5955953 , 0.02785609, 0.04227114,\n",
        "       0.28696007, 0.0057042, 0.6233334 , 0.18193275, 0.07893028,\n",
        "       0.00928628, 0.41068771, 0.5269194 , 0.077115  , 0.00308907,\n",
        "       0.54416113, 0.12486744, 0.64642929, 0.27404033, 0.38526039,\n",
        "       0.27368472, 0.96800706, 0.49461555, 0.14509363, 0.0461658,\n",
        "       0.0007261, 0.58272264, 0.02501718, 0.09205833, 0.57803194,\n",
        "       0.76988452, 0.5680329 , 0.45396565, 0.38166771, 0.06963406,\n",
        "       0.23581046, 0.3225289 , 0.8547721 , 0.63443332, 0.03894686,\n",
        "       0.62706277, 0.35008823, 0.24922772, 0.72962402, 0.84872948,\n",
        "       0.82414566, 0.20067363, 0.37857999, 0.62977724, 0.0005504,\n",
        "       0.31590734, 0.4263561 , 0.0009078, 0.00180797, 0.79175501,\n",
        "       0.9124886 , 0.47129693, 0.84219809, 0.64118798, 0.25942479,\n",
        "       0.00109813, 0.93798016, 0.48571054, 0.94116676, 0.00439978,\n",
        "       0.79443381, 0.53468295, 0.38246722, 0.53655595, 0.02342969,\n",
        "       0.41306335, 0.63949623, 0.003028193, 0.30213487, 0.20940324,\n",
        "       0.30791922, 0.82000972, 0.62882809, 0.0021391 , 0.69611787,\n",
        "       0.005386676, 0.83363883, 0.24132303, 0.37158356, 0.34748915,\n",
        "       0.07166326, 0.61643089, 0.00097506, 0.00103997, 0.4072646]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IihYiu2SKYK-"
      },
      "source": [
        "#### 1.2.1 If alpha = 10%, how many experiments would correspond to a discovery (without correction)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZqzLgfMKYK-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LYoA-HKYK-"
      },
      "source": [
        "#### 1.2.2 Correct for multiple comparisons using **Bonferroni correction**. How many discoveries you made after the correction and what are the correspondng p-values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRVj4AM0KYK-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29xHaEqhKYK-"
      },
      "source": [
        "#### 1.2.3 Correct for multiple comparisons using **Holm-Bonferroni correction**. How many discoveries you made after the correction and what are the correspondng p-values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfuYruo0KYK-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhdwDWcjKYK-"
      },
      "source": [
        "#### 1.2.4  Correct for multiple comparisons using **False Discovery Rate**. How many discoveries you made after the correction and what are the correspondng p-values? Assume the maxinum false discovery rate Q to be the same as alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkWwKHnvKYK-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-pnH8kFKYK-"
      },
      "source": [
        "#### 1.2.5 Comment on your observations. How do these three methods compare with each other?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R8HBIKg9go3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Dealing with Missing Data"
      ],
      "metadata": {
        "id": "SYPNMk-INta2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvC0ToUWKYLA"
      },
      "source": [
        "A self-driving company is testing a new concept for a self-driving car. As part of the pilot test, there is a human inside the vehicle every time the vehicle goes on for a trip. The vehicle is equipped with sensors that collect different data streams. Every time a trip is over, a computer collects data from the vehicle's sensors and some input from the human. The resulting dataset is then loaded into a database, where it becomes available for analysis to other data scientists downstream.\n",
        "\n",
        "The data scientist who used to work with the database has left the company. Coincidentally, the database is down, so you cannot access it directly. Fortunately, the previous scientist did a good job in hardening the *pipeline* and there exists a backup copy of the database as a csv file (`travel-times.csv`).\n",
        "\n",
        "Before leaving, the previous data scientist left a brief note: \"there are some missing values in this dataset\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itqs5Fl1KYLA"
      },
      "source": [
        "#### 2.1 Load the dataset `travel-times.csv`. How many cells have a missing value? Note the pipeline of stages here, from the moment where the data is generated to the moment where it gets to you is a bit more complex than in the past. Think carefully about what may go 'wrong' while the data flows through that pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkI84vCrKYLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIS1TMkdKYLB"
      },
      "source": [
        "#### 2.2 We want to understand what is a typical distance travelled. We know the car reliably measures the 'Distance' travelled in kilometers. This data, however, cannot be directly fed into the database, so the analyst reads it and inputs it manually. However, the analyst apparently forgets inputting the data sometimes, leading to some missing values. Before analyzing the results, fix the data in the 'Distance' column using the following methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq_gmgA_KYLB"
      },
      "source": [
        "##### 2.2.1 Fix 'Distance' by dropping missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpwEEH9vKYLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWZWtK2KYLB"
      },
      "source": [
        "##### 2.2.2 Fix 'Distance' by using LOCF (last observation carried forward) hot-deck imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asqyepLqKYLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xU4GC9_KYLB"
      },
      "source": [
        "##### 2.2.3 Fix 'Distance' by using mean imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h3NZiV_KYLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_WxRtD-KYLB"
      },
      "source": [
        "##### 2.2.4 Fix 'Distance' by using multiple imputation. We suggest 'MICE', which has an implementation (IterativeImputer) with sklearn, [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PtfWseYKYLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1IFhnB9KYLB"
      },
      "source": [
        "#### 2.3 All the above methods are common ways of dealing with missing values. Step back and concentrate on the original problem in 2.2. How would you fix the missing values? Which approach would you choose if you had to show this data to a decision-maker and you want to make the case that you have a reasonable and *interpretable* method? (You cannot drop values in this case and you may think of approaches beyond the ones in 2.2.1 - 2.2.4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2PGkxluKYLB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ldm1omDKYLB"
      },
      "source": [
        "#### 2.4 After deciding an imputation strategy, how would you present the 'fixed' dataset to the decision-maker? Note the decision-make may use the dataset directly, or have more components of a pipeline in use. Create the dataset you would present the decision maker and show it here along with a brief description of what you did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSd3qSnfKYLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3f9jKooKYLB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Intro to ML"
      ],
      "metadata": {
        "id": "0rpoWM3zPZvx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "yTO77VLgKYLC"
      },
      "source": [
        "## Problem 3.1 - Supervised Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "afuF9jMIKYLD"
      },
      "source": [
        "#### Machine learning starts with data. For this assignment we will be using the UCI income dataset `train.csv`. This is data collected from the US census in 1994. It contains demographic data paired with information about whether the person in the entry has annual income over $50,000. While this data is of questionable use for solving data science problems (why?), it is commonly used as a benchmark for machine learning tasks. You can read about what each of the variables in the dataset means [here](http://archive.ics.uci.edu/ml/datasets/Census+Income). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5zt6Y_m5KYLD"
      },
      "source": [
        "#### 3.1.1  Before we use this data we'll need to do some preprocessing. First you need to handle missing values. In this case you can simply drop them.\n",
        "\n",
        "#### In addition to making sure that the numeric data is properly encoded, there are several categorical variables in this data set. For example, ``workclass`` contains information about what sector each person is employed in. If we encode this by saying that State-gov=0, Self-emp-not-inc=1, etc. certain kinds of models (e.g. logistic regressions) will treat this as a statement that there is an *ordering* of these variables. The way to avoid this is by doing something called [**dummy-coding**](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-dummy-coding/). Use pandas [`get_dummies`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) API to convert all categorical columns into dummy coded columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9yCbQQ6NKYLD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "abTjMvtWKYLD"
      },
      "source": [
        "#### 3.1.2 This is technically enough processing to build a model. Lets start by using the Python library Scikit-lean (*sklearn*) to create a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). This classifier should predict whether a person has an income above $50,000. Carefully examine and modify the code below based on the comment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sgfZOr_qKYLD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wj2fg7HJKYLD"
      },
      "outputs": [],
      "source": [
        "# change df_dummy to the dataframe with dummy coded columns you created in 3.1.1\n",
        "X = df_dummy.drop(\"income_>50K\",axis=1)\n",
        "y = df_dummy[\"income_>50K\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f-0KaqfdKYLD"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "v90D17miKYLD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "v1xaOM7pKYLD"
      },
      "outputs": [],
      "source": [
        "def platform_preprocess(X_train, X_test):\n",
        "    # preprocess data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_X_train = scaler.fit_transform(X_train)\n",
        "    scaled_X_test = scaler.transform(X_test)\n",
        "    return scaled_X_train, scaled_X_test\n",
        "\n",
        "def platform_train_process(X_train, y_train):    \n",
        "    # model selection and training\n",
        "    parameters_for_testing = {\n",
        "    \"n_estimators\"    : [100,150,200] ,\n",
        "     \"max_features\"        : [3,4],\n",
        "    }\n",
        "    model = RandomForestClassifier()\n",
        "    kfold = KFold(n_splits=10, random_state=None)  \n",
        "    grid_cv = GridSearchCV(estimator=model, param_grid=parameters_for_testing, scoring='accuracy', cv=kfold)\n",
        "    result = grid_cv.fit(X_train, y_train)\n",
        "    print(\"Best: {} using {}\".format(result.best_score_, result.best_params_))\n",
        "    \n",
        "    # model training\n",
        "    tuned_model = RandomForestClassifier(n_estimators=result.best_params_['n_estimators'],\n",
        "                                         max_features=result.best_params_['max_features'])\n",
        "    tuned_model.fit(X_train, y_train)\n",
        "    \n",
        "    return tuned_model\n",
        "\n",
        "def platform_test_model(model, X_test, y_test):\n",
        "    # prediction on test data (benchmark)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vGQDVC9jKYLD"
      },
      "outputs": [],
      "source": [
        "pp_X_train, pp_X_test = platform_preprocess(X_train, X_test)\n",
        "# train\n",
        "model = platform_train_process(pp_X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "id": "nNk41L2sKYLD"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "accuracy = platform_test_model(model, pp_X_test, y_test)\n",
        "print(\"Acc: \" + str(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-5kZUoI9KYLD"
      },
      "source": [
        "#### What does the function `train_test_split` do and why should we use it? Report the train and test accuracy that were printed and interpret them in a few sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VheDQHF7KYLD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "trVa3gdzKYLD"
      },
      "source": [
        "#### 3.1.3 There are many different kinds of models you can use (e.g., decision tree, logistic regression, neural network) to build a classifier. Scikit-learn implements many of the most popular model classes. Pick one other model class and train a supervised learning model you believe to be reasonably generalizable using the same data. If you've never built models before, feel encouraged to do some searching through the Scikit-learn documentation or other tutorials to see how to build other models. Note that you may need to apply some transformations to the data depending on the model class you choose and you need to use some form of cross validation to prevent overfitting. Report metrics such as train / test accuracy. How does it compare to random forest classifier above?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Rv1GYGVWKYLD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Fairness and Interpretability\n"
      ],
      "metadata": {
        "id": "HUoj2sINRTrN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "AX19yP2BKYLE"
      },
      "source": [
        "For this assignment we will be using the UCI income data set (`income.csv`) that you used in Part 3 with few additional columns. As you may recall, this is data collected from the US census in 1994. It contains demographic data paired with information about whether the person in the entry has annual income over $50,000.\n",
        "\n",
        "In this dataset, some variables are considered \"sensitive attributes\" (protected attributes in some cases). These are generally regarded as inappropriate bases on which to make decisions because of the potential for discrimination.\n",
        "\n",
        "**Before you start:** Make sure you understand the concept of a function, and that you can use it. We provided instructions in PA 0 with a few links to resources you can use to learn what these are. In this part, you should use them extensively. If you don't, you'll find yourself spending a long amount of time writing long, tedious Python code. If you use them effectively though, you'll find yourself *reusing common functionality across problems* and saving a lot of time as a consequence. \n",
        "\n",
        "**You can use libraries, such as [fairlearn](https://fairlearn.org), to compute the fairness metrics, but make sure you understand how the metrics are computed and how to use the relevant APIs with the right input and output.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.0 We'll be working with the *two* models from Part 3 (the random forest model we gave you and the alternative one you built). Retrieve those (as functions or replicate your code here), and ensure you can read/preprocess the data, train them, and make inferences. You can train the model using the full data instead of splitting it into train and test data. You can use the same features you used in Part 3, even though the dataset we provide you in this part may have a few additional columns (so you can just ignore those until we use them explicitly)."
      ],
      "metadata": {
        "id": "6qh1Nf3lont7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7pnG8qDUKYLE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1 To start, for each of the models, obtain the predicted labels `y_pred`, and plot the false positive and false negative rates by different genders and races. You may want to use [grouped bar chart](https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html). Describe any patterns you see."
      ],
      "metadata": {
        "id": "whp4RCDtn2F-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WP7zwp-bn1NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_vhL8JhhKYLF"
      },
      "source": [
        "#### 4.2 One proposed fairness definition is **disparate impact**. Disparate impact is defined as the ratio ``Pr[Y_pred = 1 | A = 1] / Pr[Y_pred = 1 | A = 0]``. If this ratio is less than some threshold ``tau``, it is considered to have disparate impact. In this definition A = 1 when the person belongs to the group we're worried about being discriminated against and A = 0 when the person belongs to any other groups.\n",
        "\n",
        "#### Evaluate the two models using this measurement by looking at different groups in gender and race. Explain the results you see and interpret them in the context of the metric. Can you identify any particular groups that the models seem to disproportionately give positive predictions to? How would it affect the model's fairness? Give a brief answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hn_RFp5-KYLF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SoNF6CFkKYLF"
      },
      "source": [
        "#### 4.3 Another proposed fairness definition is **equalized odds**. For binary classifiers, this requires that `Pr[Y_pred = 1 | A = 0, Y = a] = Pr[Y_pred = 1 | A = 1, Y = a]` for a in {0, 1}. \n",
        "\n",
        "#### For each model, obtain the values from both sides of the equation with A corresponding to gender. Are they equal (for Y = 0 or Y = 1)? If not, What are the differences? Perform the same calculations with A corresponding to Black/Non-black races. What do these numbers say about the two models you have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "EPsX7MrAKYLF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "C8PukmFZKYLF"
      },
      "source": [
        "#### 4.4 The column ``predictions`` in the dataset we distribute in this Part 4 contains predictions from a black-box model we have constructed. Examine the predictions in light of gender and Black/Non-black races. Do they seem fair by the two fairness metrics you've seen in 4.2 and 4.3?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5ij7czR0KYLF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SQsy7_OwKYLF"
      },
      "source": [
        "#### 4.5 \"Intersectionality\" is a term coined by Kimberlé Crenshaw used to refer to the phenomena in anti-discrimination law where plaintiffs who brought cases alleging discrimination on two bases (e.g., sex and race) would lose cases because the sex-based discrimination claims and the race-based discrimination claims would be evaluated separately, rather than considering the intersection between different demographic categories. For example, black women alleging discrimination in a seniority system were denied relief because the seniority system did not disadvantage black employees (compared to all non-black employees) when ignoring gender. It also did not disadvantage women (compared to all non-women) when ignoring race. However, the seniority system **did** discriminate against the intersectional category of black women (compared to all data subjects who were not black women). \n",
        "\n",
        "#### Analyze the accuracy of the two models through this intersectionality lens. Since the number of comparisons to make grows fast with the number of intersectional groupings, please choose two intersectional groupings you consider to be of particular importance and evaluate the two models with respect to those groupings. What do you conclude? Give a brief response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nR_oVT2JKYLF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Differential Privacy"
      ],
      "metadata": {
        "id": "12UxWIPgWeT-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OIvCHw38KYLG"
      },
      "source": [
        "## Problem 5.0 - K-anonymity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YCinWY-GKYLG"
      },
      "source": [
        "[K-anonymity](https://en.wikipedia.org/wiki/K-anonymity) is a privacy concept - a dataset satisfies k-anonymity if every individual appearing in the dataset cannot be distinguished from at least k-1 other individuals in the dataset. K-anonymization can prevent re-identification attack since the attackers cannot identify a single individual out of k records that all share the same [quasi-identifier](https://en.wikipedia.org/wiki/Quasi-identifier) attributes. However, one of the biggest caveat of k-anonymization is that the privacy protection it offers can degrade drastically when dealing with *multiple releases*.\n",
        "\n",
        "Consider the following two datasets containing patients records from two different hospitals. In both datasets, zipcode, age, nationality are considered non-sensitive quasi-identifiers in this dataset, and condition is a sensitive attribute that should be excluded. The records from hospital A is 4-anonymous and the records from hospital B is 6-anonymous (double check this is the case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fGxd-l75KYLG",
        "outputId": "c4d218c0-9566-4d38-c4bd-8ad021d98936"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zipcode</th>\n",
              "      <th>age</th>\n",
              "      <th>nationality</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 30</td>\n",
              "      <td>*</td>\n",
              "      <td>COVID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 30</td>\n",
              "      <td>*</td>\n",
              "      <td>Heart Disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 30</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 30</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 40</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 40</td>\n",
              "      <td>*</td>\n",
              "      <td>Heart Disease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 40</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 40</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>606**</td>\n",
              "      <td>3*</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>606**</td>\n",
              "      <td>3*</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>606**</td>\n",
              "      <td>3*</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>606**</td>\n",
              "      <td>3*</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   zipcode     age  nationality         condition\n",
              "0    606**    < 30            *             COVID\n",
              "1    606**    < 30            *     Heart Disease\n",
              "2    606**    < 30            *   Viral Infection\n",
              "3    606**    < 30            *   Viral Infection\n",
              "4    606**   >= 40            *            Cancer\n",
              "5    606**   >= 40            *     Heart Disease\n",
              "6    606**   >= 40            *   Viral Infection\n",
              "7    606**   >= 40            *   Viral Infection\n",
              "8    606**      3*            *            Cancer\n",
              "9    606**      3*            *            Cancer\n",
              "10   606**      3*            *            Cancer\n",
              "11   606**      3*            *            Cancer"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv(\"data/Part5/hospital_A.csv\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Kko6iELZKYLG",
        "outputId": "cd0f005c-7a51-4a6b-aa49-482da88fd7f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zipcode</th>\n",
              "      <th>age</th>\n",
              "      <th>nationality</th>\n",
              "      <th>condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>COVID</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Tuberculosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Flu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Tuberculosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>606**</td>\n",
              "      <td>&lt; 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Tuberculosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>606**</td>\n",
              "      <td>&gt;= 35</td>\n",
              "      <td>*</td>\n",
              "      <td>Viral Infection</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   zipcode     age  nationality         condition\n",
              "0    606**    < 35            *             COVID\n",
              "1    606**    < 35            *      Tuberculosis\n",
              "2    606**    < 35            *               Flu\n",
              "3    606**    < 35            *      Tuberculosis\n",
              "4    606**    < 35            *            Cancer\n",
              "5    606**    < 35            *            Cancer\n",
              "6    606**   >= 35            *            Cancer\n",
              "7    606**   >= 35            *            Cancer\n",
              "8    606**   >= 35            *            Cancer\n",
              "9    606**   >= 35            *      Tuberculosis\n",
              "10   606**   >= 35            *   Viral Infection\n",
              "11   606**   >= 35            *   Viral Infection"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = pd.read_csv(\"data/Part5/hospital_B.csv\")\n",
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "j0e9xJy4KYLG"
      },
      "source": [
        "#### 5.0.1 If Alice visited both hospitals, and she is 28, can you deduce Alice’s medical condition from the combination of the two datasets? Does the combined dataset still satisfy k-anonymity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3nHsz4rJKYLG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a6P13ZsaKYLG"
      },
      "source": [
        "## Problem 5.1 - Randomized Response\n",
        "\n",
        "Differential privacy is a mathematical criterion for quantifying and preserving privacy during data analysis. In this assignment you will be asked to build up an implementation of certain analyses that achieve (epsilon, delta) differential privacy."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.1 The origin of differential privacy is in **randomized response**. In a randomized response protocol, the person taking a survey is asked to privately flip a coin. If the coin lands on heads, then the person should answer the yes or no question truthfully. If the coin lands on tails, then the person should privately flip the coin again and respond yes if heads and no if tails.\n",
        "\n",
        "#### Write a program to simulate this process with a population of 1000 people. Run the simulation 100 times where the percentage of people for whom the *true* answer to the survey question is yes is 1%, 10%, 25% and 50%. For example, if the percentage is 1%, then there should be 10 people whose true answer is yes. You can use methods such as [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) to generate the actual answers based on the coin flips (for this question consider the coin to be fair with probability of 0.5 of flipping heads or tails). We provide you with some skeleton code to help you get started.\n",
        "\n",
        "#### For each percentage, plot a histogram with the x-axis being the proportion of yes responses and y-axis being the number of runs. Also compute the expected probability of answering yes for each percentage. What do you notice about the distribution in relation to the probability you just computed? Write a few sentences describing what you see."
      ],
      "metadata": {
        "id": "yllx5JNKpds5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7e00y58AKYLG"
      },
      "outputs": [],
      "source": [
        "def simulate(percentage, n_pop=1000, coin_prob=0.5): # assign default values to some parameters\n",
        "    # fill in your code here\n",
        "    # returns the proportion of yes responses out of n_pop (ie. num of yes responses / 1000)\n",
        "    return \n",
        "\n",
        "# a list containing the simulation result for each percentage\n",
        "simulations = []\n",
        "\n",
        "# iterate over different percentages\n",
        "for percentage in [0.01, 0.1, 0.25, 0.5]:\n",
        "    # simulate result for the current percentage\n",
        "    simulation = []\n",
        "    # simulate 100 times\n",
        "    for _ in range(100):\n",
        "        # run the simulate function and add the result to list\n",
        "        proportion = simulate(percentage)\n",
        "        simulation.append(proportion)\n",
        "    # add the simulation result to the simulations list\n",
        "    simulations.append(simulation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot histograms using results from the simulations list"
      ],
      "metadata": {
        "id": "cZOMT55vp9nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PO9LDOVEKYLG"
      },
      "source": [
        "#### 5.1.2 In the randomized response, the coin flip was parameterized at 1/2 probability of answering truthfully. Try biasing the coin so that the probability of deciding to *not* answer truthfully is 0, 1/8, 1/4 and 1/2. Run the simulation again this time choosing the percentage of true yes answers equal to 1/4, and make histograms similar to the ones made in 5.1.1. Note that the probability of responding yes given a decision to produce a fake answer (when the first coin lands on tail) should still be 1/2. You should reuse the `simulate` function in 5.1.1 (notice the function parameters include everything you need to vary!)\n",
        "\n",
        "#### Do you notice any pattern in the histograms? Specifically, as the coin probability decreases from 1/2 to 0, how does the distribution change?\n",
        "\n",
        "#### Calculate the standard deviation of each simulation's distribution (of the proportion of yes responses). How does the standard deviation change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pvj2en73KYLG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e9umSw3iKYLG"
      },
      "source": [
        "## Problem 5.2 - Laplace Mechanism\n",
        "\n",
        "Differential privacy is a definition, rather than a technique. The randomized response you saw in question 1 is an example of a technique that satisfies (*ln* 3, 0) differential privacy. Another common method for achieving  differential privacy is the Laplace mechanism. \n",
        "\n",
        "The Laplace mechanism is a method for achieving differential privacy. While randomized response is distributed, in that the data aggregator finds out about only the noisy data, the Laplace mechanism is centralized. The Laplace mechanism is meant for a scenario in which a data aggregator already has the \"true\" data, but wants to release the results of queries without violating the privacy of whose data it controls. It does this by adding random noise to the output of these queries. The noise in this case is sampled from the [*Laplace distribution*](https://en.wikipedia.org/wiki/Laplace_distribution), hence the name.\n",
        "\n",
        "The key to the Laplace mechanism is the way that it chooses the scale of the distribution the noise is sampled from. The scale of the distribution is the \"spread\", how large the standard deviation is, how large and how likely you are to see significant outliers. A scale that is larger means that on average there will be more noise, and more privacy, whereas a scale that is closer to 0 will be closer to the original value and therefore less private. In the Laplace mechanism, the scale is set as the sensitivity of the query over epsilon. Recall that smaller epsilon -> more privacy, larger epsilon -> less privacy.\n",
        "\n",
        "What is the sensitivity of a query? For our purposes, we can just think of a query as a function that takes data and produces a numerical answer. The sensitivity of the query is the maximum amount that the query can differ if you give it data that is modified in a single entry. For example, let's say we want to count the number of entries in a vector that are greater than 12.6. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "mxQ7mK25KYLG"
      },
      "outputs": [],
      "source": [
        "def query(array): # our query\n",
        "    return sum(array > 12.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "t6JW5pY6KYLG"
      },
      "source": [
        "Then think about two hypothetical databases (which we can think of right now as just vectors). These databases (vectors) are *neighbors* meaning that they differ in exactly one entry. *x* and *y* in the cell below are examples of one such set of neighboring databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6I2yeuAYKYLG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.array([0, 15, 26, 35, -10, 12])\n",
        "y = np.array([12.7, 15, 26, 35, -10, 12]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6ZFJ_XpgKYLG"
      },
      "source": [
        "The difference in answers between running the query on *x* and *y* is 1. A hypothetical attacker could use information like this (or more realistically a series of slightly different queries) to recover information about people in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Yq4ncqWMKYLH",
        "outputId": "a9f73058-fb3a-468a-fe79-4fc95cb0f953"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abs(query(x) - query(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_RKBi0_uKYLH"
      },
      "source": [
        "Crucially, the most that the output of this query could be altered given any database *z* is by taking an entry of *z* that is above 12.6 to be below 12.6, or taking an entry below 12.6 to be above 12.6. In either case, this would only change the output of the query by at most 1, so the sensitivity of this query is 1. Therefore, the Laplace mechanism would add to the output of this query a value drawn from a Laplace distribution with scale `1/eps`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dIgWZUHfKYLH"
      },
      "source": [
        "#### 5.2.1 Now, suppose we have a counting query that counts the number of people answering yes to the survey as in problem 5.1 (but without the random response, so it's simply the number of truthful yes answers). What is the sensitivity of this query? Implement the Laplace mechanism for this query with your specified sensitivity, by adding noise drawm from the Laplace distribution to the output count. Like you did in problem 5.1, construct a population with percentage of true yes answers equals 1/4, and run the simulation (100 runs each) for epsilon values 10, 1, 0.1 and 0.01. You can generate Laplacian noise through the [``laplace.rvs``](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.laplace.html) function.\n",
        "\n",
        "#### Plot the distribution of laplace count for each of the four epsilon values using a [boxplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "leQbpbEBKYLH"
      },
      "outputs": [],
      "source": [
        "def laplace_count(arr, epsilon): \n",
        "    # fill in your code here\n",
        "    # Given a list of survey responses and an epsilon value, return the noisy count of yes responses\n",
        "    return\n",
        "\n",
        "# construct the arr list, run simulations by calling laplace_count 100 times for each epsilon (remember what you did in 5.1?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3g7w1vScKYLH"
      },
      "outputs": [],
      "source": [
        "# plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MOKb3OxMKYLH"
      },
      "source": [
        "#### 5.2.2 Examine the graph you just made. What do you notice about the distribution of answers as epsilon decreases? What happens when epsilon is quite small (e.g. less than 0.01?) Does this pose any problems?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qwjIX2tnKYLH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "W8fz43QsKYLH"
      },
      "source": [
        "#### 5.2.3 Compute the average laplace count (over 100 runs) for each epsilon. How much do they differ from the true count of the population? What is the standard deviation for each epsilon? What does this suggest about repeated queries under the differential privacy framework? How can a data curator/aggregator defend against repeated queries?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jv2TTOpnTIIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "m4P8qbuQKYLH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wI9KzCwZKYLH"
      },
      "source": [
        "#### 5.2.4 Counting is useful, but sometimes we also need answers to other questions like mean and median. Implement the Laplace mechanism for computing the mean of a *real valued array*. For this you will need to derive the sensitivity of the mean. That is, how much a query may vary given a difference of one entry. To do this, you will need to add as a parameter the allowable range of the list -- the maximum and minimum values that are allowable. \n",
        "\n",
        "#### You should think carefully about how to handle data that is out of the specified range. Specifically, if you drop data outside of the range, how would that affect the sensitivity analysis? Could this inadvertently reveal information meant to remain private? Is there a better way to handle data that avoids these problems?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L1fw8mXUKYLH"
      },
      "outputs": [],
      "source": [
        "def laplace_mean(arr, epsilon, min_val, max_val):\n",
        "    # fill in your code here\n",
        "    # arr is a list of real values\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BTcHQpinKYLH"
      },
      "source": [
        "##### 5.2.4(a) Using the data in the income data set (`income.csv`), plot the average difference between the true mean and the differentially private mean over 100 runs for feasible points in the grid formed by minimum and maximum age, using epsilon = 0.1. We'll do the plotting for you this time but you need to fill in the required code for computing the average difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sUL7RhFKKYLH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"data/Part5/income.csv\")\n",
        "age = df[\"age\"]\n",
        "\n",
        "ages = [17, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "grid = [(x, y) for x in ages for y in ages]\n",
        "feas_grid = [pt for pt in grid if pt[0] < pt[1]]\n",
        "\n",
        "indexer = {ages[i] : i for i in range(len(ages))}\n",
        "\n",
        "data = np.full((len(ages), len(ages)), None, dtype = float)\n",
        "\n",
        "for pt in feas_grid:\n",
        "    #  You should pass minimum age and manixum age when calling laplace_mean function\n",
        "    min_age, max_age = pt\n",
        "\n",
        "    i = indexer[min_age]\n",
        "    j = indexer[max_age]\n",
        "\n",
        "    # fill in the code here and assign data[i, j]\n",
        "    # data[i, j] sould be the average difference between the true mean and the differentially private mean over 100 runs\n",
        "\n",
        "    data[i, j] = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SXOK1TVSKYLH"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "im = ax.imshow(data, extent=[17,90,90,17])\n",
        "fig.colorbar(im)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zb6juHJNKYLH"
      },
      "source": [
        "##### 5.2.4(b) Look at magnitude and direction of the error in the plot. What does this suggest about choosing reasonable cutoffs when handling differentially private data? How might one go about minimizing this sort of error? To interpret the graph it may be useful to also plot a histogram of ages in the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7nVJzVWbKYLH"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}